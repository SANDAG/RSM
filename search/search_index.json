{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Instructions for Editing/Managing Documentation","text":"<p>TODO: update/refine as needed</p>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>To get started using Anaconda, do the following:</p> <pre><code>conda create -n rsm_docs python=3.10\nconda activate rsm_docs\npip install -r docs_requirements.txt\n</code></pre> <p>To create the website locally:</p> <pre><code>mkdocs serve\n</code></pre> <p>See the <code>mkdocs.yml</code> file for the configuration.</p> <p>To create the website on GitHub pages, see the .github/workflows/docs.yml</p>"},{"location":"api.html","title":"Application Programming Interface","text":""},{"location":"api.html#rsm.assembler.rsm_assemble","title":"<code>rsm_assemble(orig_indiv, orig_joint, rsm_indiv, rsm_joint, households, mgra_crosswalk=None, sample_rate=0.25, run_assembler=1)</code>","text":"<p>Assemble and evaluate RSM trip making.</p>"},{"location":"api.html#rsm.assembler.rsm_assemble--parameters","title":"Parameters","text":"orig_indiv <p>Trips table from \u201coriginal\u201d model run, should be comprehensive simulation of all individual trips for all synthetic households.</p> orig_joint <p>Joint trips table from \u201coriginal\u201d model run, should be comprehensive simulation of all joint trips for all synthetic households.</p> rsm_indiv <p>Trips table from RSM model run, should be a simulation of all individual trips for potentially only a subset of all synthetic households.</p> rsm_joint <p>Trips table from RSM model run, should be a simulation of all joint trips for potentially only a subset of all synthetic households (the same sampled households as in <code>rsm_indiv</code>).</p> households <p>Synthetic household file, used to get home zones for households.</p> mgra_crosswalk <p>Crosswalk from original MGRA to clustered zone ids.  Provide this crosswalk if the <code>orig_indiv</code> and <code>orig_joint</code> files reference the original MGRA system and those id\u2019s need to be converted to aggregated values before merging.</p> sample_rate <p>Default/fixed sample rate if sampler was turned off this is used to scale the trips if run_assembler is 0</p> run_assembler <p>Flag to indicate whether to run RSM assembler or not.  1 is to run assembler, 0 is to turn if off setting this to 0 is only an option if sampler is turned off       </p>"},{"location":"api.html#rsm.assembler.rsm_assemble--returns","title":"Returns","text":"final_ind_trips <p>Assembled trip table for RSM run, filling in archived trip values for non-resimulated households.</p> final_jnt_trips <p>Summary table of changes in trips by mode, by household home zone. Used to check whether undersampled zones have stable travel behavior.</p> <p>Separate tables for individual and joint trips, as required by java.</p> Source code in <code>rsm/assembler.py</code> <pre><code>def rsm_assemble(\n    orig_indiv,\n    orig_joint,\n    rsm_indiv,\n    rsm_joint,\n    households,\n    mgra_crosswalk=None,\n    sample_rate=0.25,\n    run_assembler=1\n):\n\"\"\"\n    Assemble and evaluate RSM trip making.\n\n    Parameters\n    ----------\n    orig_indiv : orig_indiv\n        Trips table from \"original\" model run, should be comprehensive simulation\n        of all individual trips for all synthetic households.\n    orig_joint : orig_joint \n        Joint trips table from \"original\" model run, should be comprehensive simulation\n        of all joint trips for all synthetic households.\n    rsm_indiv : rsm_indiv \n        Trips table from RSM model run, should be a simulation of all individual\n        trips for potentially only a subset of all synthetic households.\n    rsm_joint : rsm_joint \n        Trips table from RSM model run, should be a simulation of all joint\n        trips for potentially only a subset of all synthetic households (the\n        same sampled households as in `rsm_indiv`).\n    households : households \n        Synthetic household file, used to get home zones for households.\n    mgra_crosswalk : mgra_crosswalk \n        Crosswalk from original MGRA to clustered zone ids.  Provide this crosswalk\n        if the `orig_indiv` and `orig_joint` files reference the original MGRA system\n        and those id's need to be converted to aggregated values before merging.\n    sample_rate : sample_rate\n        Default/fixed sample rate if sampler was turned off\n        this is used to scale the trips if run_assembler is 0\n    run_assembler : run_assembler \n        Flag to indicate whether to run RSM assembler or not. \n        1 is to run assembler, 0 is to turn if off\n        setting this to 0 is only an option if sampler is turned off       \n\n    Returns\n    -------\n    final_trips_rsm : final_ind_trips\n        Assembled trip table for RSM run, filling in archived trip values for\n        non-resimulated households.\n    combined_trips_by_zone : final_jnt_trips\n        Summary table of changes in trips by mode, by household home zone.\n        Used to check whether undersampled zones have stable travel behavior.\n\n    Separate tables for individual and joint trips, as required by java.\n\n\n    \"\"\"\n    orig_indiv = Path(orig_indiv).expanduser()\n    orig_joint = Path(orig_joint).expanduser()\n    rsm_indiv = Path(rsm_indiv).expanduser()\n    rsm_joint = Path(rsm_joint).expanduser()\n    households = Path(households).expanduser()\n\n    assert os.path.isfile(orig_indiv)\n    assert os.path.isfile(orig_joint)\n    assert os.path.isfile(rsm_indiv)\n    assert os.path.isfile(rsm_joint)\n    assert os.path.isfile(households)\n\n    if mgra_crosswalk is not None:\n        mgra_crosswalk = Path(mgra_crosswalk).expanduser()\n        assert os.path.isfile(mgra_crosswalk)\n\n    # load trip data - partial simulation of RSM model\n    logger.info(\"reading ind_trips_rsm\")\n    ind_trips_rsm = pd.read_csv(rsm_indiv)\n    logger.info(\"reading jnt_trips_rsm\")\n    jnt_trips_rsm = pd.read_csv(rsm_joint)\n\n    if run_assembler == 1:\n        # load trip data - full simulation of residual/source model\n        logger.info(\"reading ind_trips_full\")\n        ind_trips_full = pd.read_csv(orig_indiv)\n        logger.info(\"reading jnt_trips_full\")\n        jnt_trips_full = pd.read_csv(orig_joint)\n\n        if mgra_crosswalk is not None:\n            logger.info(\"applying mgra_crosswalk to original data\")\n            mgra_crosswalk = pd.read_csv(mgra_crosswalk).set_index(\"MGRA\")[\"cluster_id\"]\n            mgra_crosswalk[-1] = -1\n            mgra_crosswalk[0] = 0\n            for col in [c for c in ind_trips_full.columns if c.lower().endswith(\"_mgra\")]:\n                ind_trips_full[col] = ind_trips_full[col].map(mgra_crosswalk)\n            for col in [c for c in jnt_trips_full.columns if c.lower().endswith(\"_mgra\")]:\n                jnt_trips_full[col] = jnt_trips_full[col].map(mgra_crosswalk)\n\n        # convert to rsm trips\n        logger.info(\"convert to common table platform\")\n        rsm_trips = _merge_joint_and_indiv_trips(ind_trips_rsm, jnt_trips_rsm)\n        original_trips = _merge_joint_and_indiv_trips(ind_trips_full, jnt_trips_full)\n\n        logger.info(\"get all hhids in trips produced by RSM\")\n        hh_ids_rsm = rsm_trips[\"hh_id\"].unique()\n\n        logger.info(\"remove orginal model trips made by households chosen in RSM trips\")\n        original_trips_not_resimulated = original_trips.loc[\n            ~original_trips[\"hh_id\"].isin(hh_ids_rsm)\n        ]\n        original_ind_trips_not_resimulated = ind_trips_full[\n            ~ind_trips_full[\"hh_id\"].isin(hh_ids_rsm)\n        ]\n        original_jnt_trips_not_resimulated = jnt_trips_full[\n            ~jnt_trips_full[\"hh_id\"].isin(hh_ids_rsm)\n        ]\n\n        logger.info(\"concatenate trips from rsm and original model\")\n        final_trips_rsm = pd.concat(\n            [rsm_trips, original_trips_not_resimulated], ignore_index=True\n        ).reset_index(drop=True)\n        final_ind_trips = pd.concat(\n            [ind_trips_rsm, original_ind_trips_not_resimulated], ignore_index=True\n        ).reset_index(drop=True)\n        final_jnt_trips = pd.concat(\n            [jnt_trips_rsm, original_jnt_trips_not_resimulated], ignore_index=True\n        ).reset_index(drop=True)\n\n        # Get percentage change in total trips by mode for each home zone\n\n        # extract trips made by households in RSM and Original model\n        original_trips_that_were_resimulated = original_trips.loc[\n            original_trips[\"hh_id\"].isin(hh_ids_rsm)\n        ]\n\n        def _agg_by_hhid_and_tripmode(df, name):\n            return df.groupby([\"hh_id\", \"trip_mode\"]).size().rename(name).reset_index()\n\n        # combining trips by hhid and trip mode\n        combined_trips = pd.merge(\n            _agg_by_hhid_and_tripmode(original_trips_that_were_resimulated, \"n_trips_orig\"),\n            _agg_by_hhid_and_tripmode(rsm_trips, \"n_trips_rsm\"),\n            on=[\"hh_id\", \"trip_mode\"],\n            how=\"outer\",\n            sort=True,\n        ).fillna(0)\n\n        # aggregating by Home zone\n        hh_rsm = pd.read_csv(households)\n        hh_id_col_names = [\"hhid\", \"hh_id\", \"household_id\"]\n        for hhid in hh_id_col_names:\n            if hhid in hh_rsm.columns:\n                break\n        else:\n            raise KeyError(f\"none of {hh_id_col_names!r} in household file\")\n        homezone_col_names = [\"mgra\", \"home_mgra\"]\n        for zoneid in homezone_col_names:\n            if zoneid in hh_rsm.columns:\n                break\n        else:\n            raise KeyError(f\"none of {homezone_col_names!r} in household file\")\n        hh_rsm = hh_rsm[[hhid, zoneid]]\n\n        # attach home zone id\n        combined_trips = pd.merge(\n            combined_trips, hh_rsm, left_on=\"hh_id\", right_on=hhid, how=\"left\"\n        )\n\n        combined_trips_by_zone = (\n            combined_trips.groupby([zoneid, \"trip_mode\"])[[\"n_trips_orig\", \"n_trips_rsm\"]]\n            .sum()\n            .reset_index()\n        )\n\n        combined_trips_by_zone = combined_trips_by_zone.eval(\n            \"net_change = (n_trips_rsm - n_trips_orig)\"\n        )\n\n        combined_trips_by_zone[\"max_trips\"] = np.fmax(\n            combined_trips_by_zone.n_trips_rsm, combined_trips_by_zone.n_trips_orig\n        )\n        combined_trips_by_zone = combined_trips_by_zone.eval(\n            \"pct_change = net_change / max_trips * 100\"\n        )\n        combined_trips_by_zone = combined_trips_by_zone.drop(columns=\"max_trips\")\n    else:\n        # if assembler is set to be turned off\n        # then scale the trips in the trip list using the fixed sample rate \n        # trips in the final trip lists will be 100%\n        scale_factor = int(1.0/sample_rate)\n\n        # concat is slow\n        # https://stackoverflow.com/questions/50788508/how-can-i-replicate-rows-of-a-pandas-dataframe\n        #final_ind_trips = pd.concat([ind_trips_rsm]*scale_factor, ignore_index=True)\n        #final_jnt_trips = pd.concat([jnt_trips_rsm]*scale_factor, ignore_index=True)\n\n        final_ind_trips = pd.DataFrame(\n            np.repeat(ind_trips_rsm.values, scale_factor, axis=0),\n            columns=ind_trips_rsm.columns\n        )\n\n        final_jnt_trips = pd.DataFrame(\n            np.repeat(jnt_trips_rsm.values, scale_factor, axis=0),\n            columns=jnt_trips_rsm.columns\n        )        \n\n    return final_ind_trips, final_jnt_trips\n</code></pre>"},{"location":"api.html#rsm.input_agg.agg_input_files","title":"<code>agg_input_files(model_dir='.', rsm_dir='.', taz_cwk_file='taz_crosswalk.csv', mgra_cwk_file='mgra_crosswalk.csv', agg_zones=2000, ext_zones=12, input_files=['microMgraEquivMinutes.csv', 'microMgraTapEquivMinutes.csv', 'walkMgraTapEquivMinutes.csv', 'walkMgraEquivMinutes.csv', 'bikeTazLogsum.csv', 'bikeMgraLogsum.csv', 'zone.term', 'zones.park', 'tap.ptype', 'accessam.csv', 'ParkLocationAlts.csv', 'CrossBorderDestinationChoiceSoaAlternatives.csv', 'TourDcSoaDistanceAlts.csv', 'DestinationChoiceAlternatives.csv', 'SoaTazDistAlts.csv', 'TripMatrices.csv', 'transponderModelAccessibilities.csv', 'crossBorderTours.csv', 'internalExternalTrips.csv', 'visitorTours.csv', 'visitorTrips.csv', 'householdAVTrips.csv', 'crossBorderTrips.csv', 'TNCTrips.csv', 'airport_out.SAN.csv', 'airport_out.CBX.csv', 'TNCtrips.csv'])</code>","text":""},{"location":"api.html#rsm.input_agg.agg_input_files--parameters","title":"Parameters","text":"model_dir <p>default \u201c.\u201d</p> rsm_dir <p>default \u201c.\u201d</p> taz_cwk_file <p>default taz_crosswalk.csv taz to aggregated zones file. Should be located in RSM input folder</p> mgra_cwk_file <p>default mgra_crosswalk.csv mgra to aggregated zones file. Should be located in RSM input folder</p> input_files <p>list of input files to be aggregated.  Should include the following files     \u201cmicroMgraEquivMinutes.csv\u201d, \u201cmicroMgraTapEquivMinutes.csv\u201d,      \u201cwalkMgraTapEquivMinutes.csv\u201d, \u201cwalkMgraEquivMinutes.csv\u201d, \u201cbikeTazLogsum.csv\u201d,     \u201cbikeMgraLogsum.csv\u201d, \u201czone.term\u201d, \u201czones.park\u201d, \u201ctap.ptype\u201d, \u201caccessam.csv\u201d,     \u201cParkLocationAlts.csv\u201d, \u201cCrossBorderDestinationChoiceSoaAlternatives.csv\u201d,     \u201cTourDcSoaDistanceAlts.csv\u201d, \u201cDestinationChoiceAlternatives.csv\u201d, \u201cSoaTazDistAlts.csv\u201d,     \u201cTripMatrices.csv\u201d, \u201ctransponderModelAccessibilities.csv\u201d, \u201ccrossBorderTours.csv\u201d,     \u201cinternalExternalTrips.csv\u201d, \u201cvisitorTours.csv\u201d, \u201cvisitorTrips.csv\u201d, \u201chouseholdAVTrips.csv\u201d,     \u201ccrossBorderTrips.csv\u201d, \u201cTNCTrips.csv\u201d, \u201cairport_out.SAN.csv\u201d, \u201cairport_out.CBX.csv\u201d,     \u201cTNCtrips.csv\u201d</p>"},{"location":"api.html#rsm.input_agg.agg_input_files--returns","title":"Returns","text":"<p>Aggregated files in the RSM input/output/uec directory</p> Source code in <code>rsm/input_agg.py</code> <pre><code>def agg_input_files(\n    model_dir = \".\", \n    rsm_dir = \".\",\n    taz_cwk_file = \"taz_crosswalk.csv\",\n    mgra_cwk_file = \"mgra_crosswalk.csv\",\n    agg_zones=2000,\n    ext_zones=12,\n    input_files = [\"microMgraEquivMinutes.csv\", \"microMgraTapEquivMinutes.csv\", \n    \"walkMgraTapEquivMinutes.csv\", \"walkMgraEquivMinutes.csv\", \"bikeTazLogsum.csv\",\n    \"bikeMgraLogsum.csv\", \"zone.term\", \"zones.park\", \"tap.ptype\", \"accessam.csv\",\n    \"ParkLocationAlts.csv\", \"CrossBorderDestinationChoiceSoaAlternatives.csv\", \n    \"TourDcSoaDistanceAlts.csv\", \"DestinationChoiceAlternatives.csv\", \"SoaTazDistAlts.csv\",\n    \"TripMatrices.csv\", \"transponderModelAccessibilities.csv\", \"crossBorderTours.csv\", \n    \"internalExternalTrips.csv\", \"visitorTours.csv\", \"visitorTrips.csv\", \"householdAVTrips.csv\", \n    \"crossBorderTrips.csv\", \"TNCTrips.csv\", \"airport_out.SAN.csv\", \"airport_out.CBX.csv\", \n    \"TNCtrips.csv\"]\n    ):\n\n\"\"\"\n        Parameters\n        ----------\n        model_dir : model_dir \n            default \".\"\n        rsm_dir : rsm_dir \n            default \".\"\n        taz_cwk_file : taz_cwk_file \n            default taz_crosswalk.csv\n            taz to aggregated zones file. Should be located in RSM input folder\n        mgra_cwk_file : mgra_cwk_file \n            default mgra_crosswalk.csv\n            mgra to aggregated zones file. Should be located in RSM input folder\n        input_files : input_files\n            list of input files to be aggregated. \n            Should include the following files\n                \"microMgraEquivMinutes.csv\", \"microMgraTapEquivMinutes.csv\", \n                \"walkMgraTapEquivMinutes.csv\", \"walkMgraEquivMinutes.csv\", \"bikeTazLogsum.csv\",\n                \"bikeMgraLogsum.csv\", \"zone.term\", \"zones.park\", \"tap.ptype\", \"accessam.csv\",\n                \"ParkLocationAlts.csv\", \"CrossBorderDestinationChoiceSoaAlternatives.csv\",\n                \"TourDcSoaDistanceAlts.csv\", \"DestinationChoiceAlternatives.csv\", \"SoaTazDistAlts.csv\",\n                \"TripMatrices.csv\", \"transponderModelAccessibilities.csv\", \"crossBorderTours.csv\",\n                \"internalExternalTrips.csv\", \"visitorTours.csv\", \"visitorTrips.csv\", \"householdAVTrips.csv\",\n                \"crossBorderTrips.csv\", \"TNCTrips.csv\", \"airport_out.SAN.csv\", \"airport_out.CBX.csv\",\n                \"TNCtrips.csv\"\n\n        Returns\n        -------\n        Aggregated files in the RSM input/output/uec directory\n    \"\"\"\n\n    df_clusters = pd.read_csv(os.path.join(rsm_dir, \"input\", taz_cwk_file))\n    df_clusters.columns= df_clusters.columns.str.strip().str.lower()\n    dict_clusters = dict(zip(df_clusters['taz'], df_clusters['cluster_id']))\n\n    mgra_cwk = pd.read_csv(os.path.join(rsm_dir, \"input\", mgra_cwk_file))\n    mgra_cwk.columns= mgra_cwk.columns.str.strip().str.lower()\n    mgra_cwk = dict(zip(mgra_cwk['mgra'], mgra_cwk['cluster_id']))\n\n    taz_zones = int(agg_zones) + int(ext_zones)\n    mgra_zones = int(agg_zones)\n\n    # aggregating microMgraEquivMinutes.csv\n    if \"microMgraEquivMinutes.csv\" in input_files:\n        logging.info(\"Aggregating - microMgraEquivMinutes.csv\")\n        df_mm_eqmin = pd.read_csv(os.path.join(model_dir, \"output\", \"microMgraEquivMinutes.csv\"))\n        df_mm_eqmin['i_new'] = df_mm_eqmin['i'].map(mgra_cwk)\n        df_mm_eqmin['j_new'] = df_mm_eqmin['j'].map(mgra_cwk)\n\n        df_mm_eqmin_agg = df_mm_eqmin.groupby(['i_new', 'j_new'])['walkTime', 'dist', 'mmTime', 'mmCost', 'mtTime', 'mtCost',\n       'mmGenTime', 'mtGenTime', 'minTime'].mean().reset_index()\n\n        df_mm_eqmin_agg = df_mm_eqmin_agg.rename(columns = {'i_new' : 'i', 'j_new' : 'j'})\n        df_mm_eqmin_agg.to_csv(os.path.join(rsm_dir, \"input\", \"microMgraEquivMinutes.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"microMgraEquivMinutes.csv\")\n\n\n    # aggregating microMgraTapEquivMinutes.csv\"   \n    if \"microMgraTapEquivMinutes.csv\" in input_files:\n        logging.info(\"Aggregating - microMgraTapEquivMinutes.csv\")\n        df_mm_tap = pd.read_csv(os.path.join(model_dir, \"output\", \"microMgraTapEquivMinutes.csv\"))\n        df_mm_tap['mgra'] = df_mm_tap['mgra'].map(mgra_cwk)\n\n        df_mm_tap_agg = df_mm_tap.groupby(['mgra', 'tap'])['walkTime', 'dist', 'mmTime', 'mmCost', 'mtTime',\n       'mtCost', 'mmGenTime', 'mtGenTime', 'minTime'].mean().reset_index()\n\n        df_mm_tap_agg.to_csv(os.path.join(rsm_dir, \"input\", \"microMgraTapEquivMinutes.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"microMgraTapEquivMinutes.csv\")\n\n    # aggregating walkMgraTapEquivMinutes.csv\n    if \"walkMgraTapEquivMinutes.csv\" in input_files:\n        logging.info(\"Aggregating - walkMgraTapEquivMinutes.csv\")\n        df_wlk_mgra_tap = pd.read_csv(os.path.join(model_dir, \"output\", \"walkMgraTapEquivMinutes.csv\"))\n        df_wlk_mgra_tap[\"mgra\"] = df_wlk_mgra_tap[\"mgra\"].map(mgra_cwk)\n\n        df_wlk_mgra_agg = df_wlk_mgra_tap.groupby([\"mgra\", \"tap\"])[\"boardingPerceived\", \"boardingActual\",\"alightingPerceived\",\"alightingActual\",\"boardingGain\",\"alightingGain\"].mean().reset_index()\n        df_wlk_mgra_agg.to_csv(os.path.join(rsm_dir, \"input\", \"walkMgraTapEquivMinutes.csv\"), index = False)\n\n    else:\n        FileNotFoundError(\"walkMgraTapEquivMinutes.csv\")\n\n    # aggregating walkMgraEquivMinutes.csv\n    if \"walkMgraEquivMinutes.csv\" in input_files:\n        logging.info(\"Aggregating - walkMgraEquivMinutes.csv\")\n        df_wlk_min = pd.read_csv(os.path.join(model_dir, \"output\", \"walkMgraEquivMinutes.csv\"))\n        df_wlk_min[\"i\"] = df_wlk_min[\"i\"].map(mgra_cwk)\n        df_wlk_min[\"j\"] = df_wlk_min[\"j\"].map(mgra_cwk)\n\n        df_wlk_min_agg = df_wlk_min.groupby([\"i\", \"j\"])[\"percieved\",\"actual\", \"gain\"].mean().reset_index()\n\n        df_wlk_min_agg.to_csv(os.path.join(rsm_dir, \"input\", \"walkMgraEquivMinutes.csv\"), index = False)\n\n    else:\n        FileNotFoundError(\"walkMgraEquivMinutes.csv\")\n\n    # aggregating biketazlogsum\n    if \"bikeTazLogsum.csv\" in input_files:\n        logging.info(\"Aggregating - bikeTazLogsum.csv\")\n        bike_taz = pd.read_csv(os.path.join(model_dir, \"output\", \"bikeTazLogsum.csv\"))\n\n        bike_taz[\"i\"] = bike_taz[\"i\"].map(dict_clusters)\n        bike_taz[\"j\"] = bike_taz[\"j\"].map(dict_clusters)\n\n        bike_taz_agg = bike_taz.groupby([\"i\", \"j\"])[\"logsum\", \"time\"].mean().reset_index()\n        bike_taz_agg.to_csv(os.path.join(rsm_dir, \"input\", \"bikeTazLogsum.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"bikeTazLogsum.csv\")\n\n    # aggregating bikeMgraLogsum.csv\n    if \"bikeMgraLogsum.csv\" in input_files:\n        logging.info(\"Aggregating - bikeMgraLogsum.csv\")\n        bike_mgra = pd.read_csv(os.path.join(model_dir, \"output\", \"bikeMgraLogsum.csv\"))\n        bike_mgra[\"i\"] = bike_mgra[\"i\"].map(mgra_cwk)\n        bike_mgra[\"j\"] = bike_mgra[\"j\"].map(mgra_cwk)\n\n        bike_mgra_agg = bike_mgra.groupby([\"i\", \"j\"])[\"logsum\", \"time\"].mean().reset_index()\n        bike_mgra_agg.to_csv(os.path.join(rsm_dir, \"input\", \"bikeMgraLogsum.csv\"), index = False)\n    else:\n        raise FileNotFoundError(\"bikeMgraLogsum.csv\")\n\n    # aggregating zone.term\n    if \"zone.term\" in input_files:\n        logging.info(\"Aggregating - zone.term\")\n        df_zone_term = pd.read_fwf(os.path.join(model_dir, \"input\", \"zone.term\"), header = None)\n        df_zone_term.columns = [\"taz\", \"terminal_time\"]\n\n        df_agg = pd.merge(df_zone_term, df_clusters, on = \"taz\", how = 'left')\n        df_zones_agg = df_agg.groupby([\"cluster_id\"])['terminal_time'].max().reset_index()\n\n        df_zones_agg.columns = [\"taz\", \"terminal_time\"]\n        df_zones_agg.to_fwf(os.path.join(rsm_dir, \"input\", \"zone.term\"))\n\n    else:\n        raise FileNotFoundError(\"zone.term\")\n\n    # aggregating zones.park\n    if \"zones.park\" in input_files:\n        logging.info(\"Aggregating - zone.park\")\n        df_zones_park = pd.read_fwf(os.path.join(model_dir, \"input\", \"zone.park\"), header = None)\n        df_zones_park.columns = [\"taz\", \"park_zones\"]\n\n        df_zones_park_agg = pd.merge(df_zones_park, df_clusters, on = \"taz\", how = 'left')\n        df_zones_park_agg = df_zones_park_agg.groupby([\"cluster_id\"])['park_zones'].max().reset_index()\n        df_zones_park_agg.columns = [\"taz\", \"park_zones\"]\n        df_zones_park_agg.to_fwf(os.path.join(rsm_dir, \"input\", \"zone.park\"))\n\n    else:\n        raise FileNotFoundError(\"zone.park\")\n\n\n    # aggregating tap.ptype \n    if \"tap.ptype\" in input_files:\n        logging.info(\"Aggregating - tap.ptype\")\n        df_tap_ptype = pd.read_fwf(os.path.join(model_dir, \"input\", \"tap.ptype\"), header = None)\n        df_tap_ptype.columns = [\"tap\", \"lot id\", \"parking type\", \"taz\", \"capacity\", \"distance\", \"transit mode\"]\n\n        df_tap_ptype = pd.merge(df_tap_ptype, df_clusters, on = \"taz\", how = 'left')\n\n        df_tap_ptype = df_tap_ptype[[\"tap\", \"lot id\", \"parking type\", \"cluster_id\", \"capacity\", \"distance\", \"transit mode\"]]\n        df_tap_ptype = df_tap_ptype.rename(columns = {\"cluster_id\": \"taz\"})\n        #df_tap_ptype.to_fwf(os.path.join(rsm_dir, \"input\", \"tap.ptype\"))\n\n        widths = [5, 6, 6, 5, 5, 5, 3]\n\n        with open(os.path.join(rsm_dir, \"input\", \"tap.ptype\"), 'w') as f:\n            for index, row in df_tap_ptype.iterrows():\n                field1 = str(row[0]).rjust(widths[0])\n                field2 = str(row[1]).rjust(widths[1])\n                field3 = str(row[2]).rjust(widths[2])\n                field4 = str(row[3]).rjust(widths[3])\n                field5 = str(row[4]).rjust(widths[4])\n                field6 = str(row[5]).rjust(widths[5])\n                field7 = str(row[6]).rjust(widths[6])\n                f.write(f'{field1}{field2}{field3}{field4}{field5}{field6}{field7}\\n')\n\n    else:\n        raise FileNotFoundError(\"tap.ptype\")\n\n    #aggregating accessam.csv\n    if \"accessam.csv\" in input_files:\n        logging.info(\"Aggregating - accessam.csv\")\n        df_acc = pd.read_csv(os.path.join(model_dir, \"input\", \"accessam.csv\"), header = None)\n        df_acc.columns = ['TAZ', 'TAP', 'TIME', 'DISTANCE', 'MODE']\n\n        df_acc['TAZ'] = df_acc['TAZ'].map(dict_clusters)\n        df_acc_agg = df_acc.groupby(['TAZ', 'TAP', 'MODE'])['TIME', 'DISTANCE'].mean().reset_index()\n        df_acc_agg = df_acc_agg[[\"TAZ\", \"TAP\", \"TIME\", \"DISTANCE\", \"MODE\"]]\n\n        df_acc_agg.to_csv(os.path.join(rsm_dir, \"input\", \"accessam.csv\"), index = False, header =False)\n    else:\n        raise FileNotFoundError(\"accessam.csv\")\n\n    # aggregating ParkLocationAlts.csv\n    if \"ParkLocationAlts.csv\" in input_files:\n        logging.info(\"Aggregating - ParkLocationAlts.csv\")\n        df_park = pd.read_csv(os.path.join(model_dir, \"uec\", \"ParkLocationAlts.csv\"))\n        df_park['mgra_new'] = df_park[\"mgra\"].map(mgra_cwk)\n        df_park_agg = df_park.groupby([\"mgra_new\"])[\"parkarea\"].min().reset_index() # assuming 1 is \"parking\" and 2 is \"no parking\"\n        df_park_agg['a'] = [i+1 for i in range(len(df_park_agg))]\n\n        df_park_agg.columns = [\"a\", \"mgra\", \"parkarea\"]\n        df_park_agg.to_csv(os.path.join(rsm_dir, \"uec\", \"ParkLocationAlts.csv\"), index = False)\n\n    else:\n        FileNotFoundError(\"ParkLocationAlts.csv\")\n\n    # aggregating CrossBorderDestinationChoiceSoaAlternatives.csv\n    if \"CrossBorderDestinationChoiceSoaAlternatives.csv\" in input_files:\n        logging.info(\"Aggregating - CrossBorderDestinationChoiceSoaAlternatives.csv\")\n        df_cb = pd.read_csv(os.path.join(model_dir, \"uec\",\"CrossBorderDestinationChoiceSoaAlternatives.csv\"))\n\n        df_cb[\"mgra_entry\"] = df_cb[\"mgra_entry\"].map(mgra_cwk)\n        df_cb[\"mgra_return\"] = df_cb[\"mgra_return\"].map(mgra_cwk)\n        df_cb[\"a\"] = df_cb[\"a\"].map(mgra_cwk)\n\n        df_cb = pd.merge(df_cb, df_clusters, left_on = \"dest\", right_on = \"taz\", how = 'left')\n        df_cb = df_cb.drop(columns = [\"dest\", \"taz\"])\n        df_cb = df_cb.rename(columns = {'cluster_id' : 'dest'})\n\n        df_cb_final  = df_cb.drop_duplicates()\n\n        df_cb_final = df_cb_final[[\"a\", \"dest\", \"poe\", \"mgra_entry\", \"mgra_return\", \"poe_taz\"]]\n        df_cb_final.to_csv(os.path.join(rsm_dir, \"uec\", \"CrossBorderDestinationChoiceSoaAlternatives.csv\"), index = False)\n\n    else:\n        FileNotFoundError(\"CrossBorderDestinationChoiceSoaAlternatives.csv\")\n\n    # aggregating households.csv\n    if \"households.csv\" in input_files:\n        logging.info(\"Aggregating - households.csv\")\n        df_hh = pd.read_csv(os.path.join(model_dir, \"input\", \"households.csv\"))\n        df_hh[\"mgra\"] = df_hh[\"mgra\"].map(mgra_cwk)\n        df_hh[\"taz\"] = df_hh[\"taz\"].map(dict_clusters)\n\n        df_hh.to_csv(os.path.join(rsm_dir, \"input\", \"households.csv\"), index = False)\n\n    else:\n        FileNotFoundError(\"households.csv\")\n\n    # aggregating ShadowPricingOutput_school_9.csv\n    if \"ShadowPricingOutput_school_9.csv\" in input_files:\n        logging.info(\"Aggregating - ShadowPricingOutput_school_9.csv\")\n        df_sp_sch = pd.read_csv(os.path.join(model_dir, \"input\", \"ShadowPricingOutput_school_9.csv\"))\n\n        agg_instructions = {}\n        for col in df_sp_sch.columns:\n            if \"size\" in col:\n                agg_instructions.update({col: \"sum\"})\n\n            if \"shadowPrices\" in col:\n                agg_instructions.update({col: \"max\"})\n\n            if \"_origins\" in col:\n                agg_instructions.update({col: \"sum\"})\n\n            if \"_modeledDests\" in col:\n                agg_instructions.update({col: \"sum\"})\n\n        df_sp_sch['mgra'] = df_sp_sch['mgra'].map(mgra_cwk)\n        df_sp_sch_agg = df_sp_sch.groupby(['mgra']).agg(agg_instructions).reset_index()\n\n        alt = list(df_sp_sch_agg['mgra'])\n        df_sp_sch_agg.insert(loc=0, column=\"alt\", value=alt)\n        df_sp_sch_agg.loc[len(df_sp_agg.index)] = 0\n\n        df_sp_sch_agg.to_csv(os.path.join(rsm_dir, \"input\", \"ShadowPricingOutput_school_9.csv\"), index=False)\n\n    else:\n        FileNotFoundError(\"ShadowPricingOutput_school_9.csv\")\n\n    # aggregating ShadowPricingOutput_work_9.csv\n    if \"ShadowPricingOutput_work_9.csv\" in input_files:\n        logging.info(\"Aggregating - ShadowPricingOutput_work_9.csv\")\n        df_sp_wrk = pd.read_csv(os.path.join(model_dir, \"input\", \"ShadowPricingOutput_work_9.csv\"))\n\n        agg_instructions = {}\n        for col in df_sp_wrk.columns:\n            if \"size\" in col:\n                agg_instructions.update({col: \"sum\"})\n\n            if \"shadowPrices\" in col:\n                agg_instructions.update({col: \"max\"})\n\n            if \"_origins\" in col:\n                agg_instructions.update({col: \"sum\"})\n\n            if \"_modeledDests\" in col:\n                agg_instructions.update({col: \"sum\"})\n\n        df_sp_wrk['mgra'] = df_sp_wrk['mgra'].map(mgra_cwk)\n\n        df_sp_wrk_agg = df_sp_wrk.groupby(['mgra']).agg(agg_instructions).reset_index()\n\n        alt = list(df_sp_wrk_agg['mgra'])\n        df_sp_wrk_agg.insert(loc=0, column=\"alt\", value=alt)\n\n        df_sp_wrk_agg.loc[len(df_sp_wrk_agg.index)] = 0\n\n        df_sp_wrk_agg.to_csv(os.path.join(rsm_dir, \"input\", \"ShadowPricingOutput_work_9.csv\"), index=False)\n\n    else:\n        FileNotFoundError(\"ShadowPricingOutput_work_9.csv\")\n\n    if \"TourDcSoaDistanceAlts.csv\" in input_files:\n        logging.info(\"Aggregating - TourDcSoaDistanceAlts.csv\")\n        df_TourDcSoaDistanceAlts = pd.DataFrame({\"a\" : range(1,taz_zones+1), \"dest\" : range(1, taz_zones+1)})\n        df_TourDcSoaDistanceAlts.to_csv(os.path.join(rsm_dir, \"uec\", \"TourDcSoaDistanceAlts.csv\"), index=False)\n\n    if \"DestinationChoiceAlternatives.csv\" in input_files:\n        logging.info(\"Aggregating - DestinationChoiceAlternatives.csv\")\n        df_DestinationChoiceAlternatives = pd.DataFrame({\"a\" : range(1,mgra_zones+1), \"mgra\" : range(1, mgra_zones+1)})\n        df_DestinationChoiceAlternatives.to_csv(os.path.join(rsm_dir, \"uec\", \"DestinationChoiceAlternatives.csv\"), index=False)\n\n    if \"SoaTazDistAlts.csv\" in input_files:\n        logging.info(\"Aggregating - SoaTazDistAlts.csv\")\n        df_SoaTazDistAlts = pd.DataFrame({\"a\" : range(1,taz_zones+1), \"dest\" : range(1, taz_zones+1)})\n        df_SoaTazDistAlts.to_csv(os.path.join(rsm_dir, \"uec\", \"SoaTazDistAlts.csv\"), index=False)\n\n    if \"TripMatrices.csv\" in input_files:\n        logging.info(\"Aggregating - TripMatrices.csv\")\n        trips = pd.read_csv(os.path.join(model_dir,\"output\", \"TripMatrices.csv\"))\n        trips['i'] = trips['i'].map(dict_clusters)\n        trips['j'] = trips['j'].map(dict_clusters)\n\n        cols = list(trips.columns)\n        cols.remove(\"i\")\n        cols.remove(\"j\")\n\n        trips_df = trips.groupby(['i', 'j'])[cols].sum().reset_index()\n        trips_df.to_csv(os.path.join(rsm_dir, \"output\", \"TripMatrices.csv\"), index = False)\n\n    else:\n        FileNotFoundError(\"TripMatrices.csv\")\n\n    if \"transponderModelAccessibilities.csv\" in input_files:\n        logging.info(\"Aggregating - transponderModelAccessibilities.csv\")\n        tran_access = pd.read_csv(os.path.join(model_dir, \"output\", \"transponderModelAccessibilities.csv\"))\n        tran_access['TAZ'] = tran_access['TAZ'].map(dict_clusters)\n\n        tran_access_agg = tran_access.groupby(['TAZ'])['DIST','AVGTTS','PCTDETOUR'].mean().reset_index()\n        tran_access_agg.to_csv(os.path.join(rsm_dir, \"output\",\"transponderModelAccessibilities.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"transponderModelAccessibilities.csv\")\n\n    if \"crossBorderTours.csv\" in input_files:\n        logging.info(\"Aggregating - crossBorderTours.csv\")\n        df = pd.read_csv(os.path.join(model_dir, \"output\", \"crossBorderTours.csv\"))\n        df['originMGRA'] = df['originMGRA'].map(mgra_cwk)\n        df['destinationMGRA'] = df['destinationMGRA'].map(mgra_cwk)\n\n        df['originTAZ'] = df['originTAZ'].map(dict_clusters)\n        df['destinationTAZ'] = df['destinationTAZ'].map(dict_clusters)\n        df.to_csv(os.path.join(rsm_dir, \"output\", \"crossBorderTours.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"crossBorderTours.csv\")\n\n    if \"crossBorderTrips.csv\" in input_files:\n        logging.info(\"Aggregating - crossBorderTrips.csv\")\n        df = pd.read_csv(os.path.join(model_dir, \"output\", \"crossBorderTrips.csv\"))\n        df['originMGRA'] = df['originMGRA'].map(mgra_cwk)\n        df['destinationMGRA'] = df['destinationMGRA'].map(mgra_cwk)\n\n        df['originTAZ'] = df['originTAZ'].map(dict_clusters)\n        df['destinationTAZ'] = df['destinationTAZ'].map(dict_clusters)\n        df.to_csv(os.path.join(rsm_dir, \"output\", \"crossBorderTrips.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"crossBorderTrips.csv\")\n\n    if \"internalExternalTrips.csv\" in input_files:\n        logging.info(\"Aggregating - internalExternalTrips.csv\")\n        df = pd.read_csv(os.path.join(model_dir, \"output\", \"internalExternalTrips.csv\"))\n        df['originMGRA'] = df['originMGRA'].map(mgra_cwk)\n        df['destinationMGRA'] = df['destinationMGRA'].map(mgra_cwk)\n\n        df['originTAZ'] = df['originTAZ'].map(dict_clusters)\n        df['destinationTAZ'] = df['destinationTAZ'].map(dict_clusters)\n        df.to_csv(os.path.join(rsm_dir, \"output\", \"internalExternalTrips.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"internalExternalTrips.csv\")\n\n    if \"visitorTours.csv\" in input_files:\n        logging.info(\"Aggregating - visitorTours.csv\")\n        df = pd.read_csv(os.path.join(model_dir, \"output\", \"visitorTours.csv\"))\n\n        df['originMGRA'] = df['originMGRA'].map(mgra_cwk)\n        df['destinationMGRA'] = df['destinationMGRA'].map(mgra_cwk)\n\n        df.to_csv(os.path.join(rsm_dir, \"output\", \"visitorTours.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"visitorTours.csv\")\n\n    if \"visitorTrips.csv\" in input_files:\n        logging.info(\"Aggregating - visitorTrips.csv\")\n        df = pd.read_csv(os.path.join(model_dir, \"output\", \"visitorTrips.csv\"))\n\n        df['originMGRA'] = df['originMGRA'].map(mgra_cwk)\n        df['destinationMGRA'] = df['destinationMGRA'].map(mgra_cwk)\n\n        df.to_csv(os.path.join(rsm_dir, \"output\", \"visitorTrips.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"visitorTrips.csv\")\n\n    if \"householdAVTrips.csv\" in input_files:\n        logging.info(\"Aggregating - householdAVTrips.csv\")\n        df = pd.read_csv(os.path.join(model_dir, \"output\", \"householdAVTrips.csv\"))\n        #print(os.path.join(model_dir, \"output\", \"householdAVTrips.csv\"))\n        df['orig_mgra'] = df['orig_mgra'].map(mgra_cwk)\n        df['dest_gra'] = df['dest_gra'].map(mgra_cwk)\n\n        df['trip_orig_mgra'] = df['trip_orig_mgra'].map(mgra_cwk)\n        df['trip_dest_mgra'] = df['trip_dest_mgra'].map(mgra_cwk)\n        df.to_csv(os.path.join(rsm_dir, \"output\", \"householdAVTrips.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"householdAVTrips.csv\")\n\n    if \"airport_out.CBX.csv\" in input_files:\n        logging.info(\"Aggregating - airport_out.CBX.csv\")\n        df = pd.read_csv(os.path.join(model_dir, \"output\", \"airport_out.CBX.csv\"))\n        df['originMGRA'] = df['originMGRA'].map(mgra_cwk)\n        df['destinationMGRA'] = df['destinationMGRA'].map(mgra_cwk)\n\n        df['originTAZ'] = df['originTAZ'].map(dict_clusters)\n        df['destinationTAZ'] = df['destinationTAZ'].map(dict_clusters)\n        df.to_csv(os.path.join(rsm_dir, \"output\", \"airport_out.CBX.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"airport_out.CBX.csv\")\n\n    if \"airport_out.SAN.csv\" in input_files:\n        logging.info(\"Aggregating - airport_out.SAN.csv\")\n        df = pd.read_csv(os.path.join(model_dir, \"output\", \"airport_out.SAN.csv\"))\n        df['originMGRA'] = df['originMGRA'].map(mgra_cwk)\n        df['destinationMGRA'] = df['destinationMGRA'].map(mgra_cwk)\n\n        df['originTAZ'] = df['originTAZ'].map(dict_clusters)\n        df['destinationTAZ'] = df['destinationTAZ'].map(dict_clusters)\n        df.to_csv(os.path.join(rsm_dir, \"output\", \"airport_out.SAN.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"airport_out.SAN.csv\")\n\n    if \"TNCtrips.csv\" in input_files:\n        logging.info(\"Aggregating - TNCtrips.csv\")\n        df = pd.read_csv(os.path.join(model_dir, \"output\", \"TNCtrips.csv\"))\n        df['originMgra'] = df['originMgra'].map(mgra_cwk)\n        df['destinationMgra'] = df['destinationMgra'].map(mgra_cwk)\n\n        df['originTaz'] = df['originTaz'].map(dict_clusters)\n        df['destinationTaz'] = df['destinationTaz'].map(dict_clusters)\n        df.to_csv(os.path.join(rsm_dir, \"output\", \"TNCtrips.csv\"), index = False)\n\n    else:\n        raise FileNotFoundError(\"TNCtrips.csv\")\n\n    files = [\"Trip\" + \"_\" + i + \"_\" + j + \".csv\" for i, j in\n                itertools.product([\"FA\", \"GO\", \"IN\", \"RE\", \"SV\", \"TH\", \"WH\"],\n                                   [\"OE\", \"AM\", \"MD\", \"PM\", \"OL\"])]\n\n    for file in files:\n        logging.info(f\"Aggregating - {file}\")\n        df = pd.read_csv(os.path.join(model_dir, \"output\", file))\n        df['I'] = df['I'].map(dict_clusters)\n        df['J'] = df['J'].map(dict_clusters)\n        df['HomeZone'] = df['HomeZone'].map(dict_clusters)\n        df.to_csv(os.path.join(rsm_dir, \"output\",file), index = False)\n</code></pre>"},{"location":"api.html#rsm.poi.attach_poi_taz_skims","title":"<code>attach_poi_taz_skims(gdf, skims_omx, names, poi=None, data_dir=None, taz_col='taz', cluster_factors=None)</code>","text":"<p>Attach TAZ-based skim values to rows of a geodataframe.</p>"},{"location":"api.html#rsm.poi.attach_poi_taz_skims--parameters","title":"Parameters","text":"gdf <p>The skimmed values will be added as columns to this [geo]dataframe. If the POI\u2019s are given explicitly, this could be a regular pandas DataFrame, otherwise the geometry is used to find the TAZ\u2019s of the points of interest.</p> skims_omx <p>Openmatrix.File of skimmed values.</p> names <p>Keys give the names of matrix tables to load out of the skims file. Values give the relative weight for each table (used later in clustering).</p> poi <p>Maps named points of interest to the \u2018taz\u2019 id of each.  If not given, these will be computed based on the <code>gdf</code>.</p> data_dir, optional <p>Directory where the <code>skims_omx</code> file can be found, if not the current working directory.</p> cluster_factors, optional <p>Existing cluster_factors, to which the new factors are added.</p>"},{"location":"api.html#rsm.poi.attach_poi_taz_skims--returns","title":"Returns","text":"gdf <p>[geo]dataframe to which the TAZ\u2019s of the points of interest were added.</p> cluster_factors <p>Resulting cluster_factors.</p> Source code in <code>rsm/poi.py</code> <pre><code>def attach_poi_taz_skims(\n    gdf, skims_omx, names, poi=None, data_dir=None, taz_col=\"taz\", cluster_factors=None\n):\n\"\"\"\n    Attach TAZ-based skim values to rows of a geodataframe.\n\n    Parameters\n    ----------\n    gdf : gdf\n        The skimmed values will be added as columns to this [geo]dataframe.\n        If the POI's are given explicitly, this could be a regular pandas\n        DataFrame, otherwise the geometry is used to find the TAZ's of the\n        points of interest.\n    skims_omx : skims_omx\n        Openmatrix.File of skimmed values.\n    names : names\n        Keys give the names of matrix tables to load out of the skims file.\n        Values give the relative weight for each table (used later in\n        clustering).\n    poi : poi\n        Maps named points of interest to the 'taz' id of each.  If not given,\n        these will be computed based on the `gdf`.\n    data_dir : data_dir, optional\n        Directory where the `skims_omx` file can be found, if not the current\n        working directory.\n    cluster_factors : cluster_factors, optional\n        Existing cluster_factors, to which the new factors are added.\n\n    Returns\n    -------\n    gdf : gdf\n        [geo]dataframe to which the TAZ's of the points of interest were added.\n    cluster_factors : cluster_factors\n        Resulting cluster_factors.\n    \"\"\"\n    if poi is None:\n        poi = poi_taz_mgra(gdf)\n    if isinstance(names, str):\n        names = {names: 1.0}\n    if isinstance(skims_omx, (str, Path)):\n        skims_omx = open_skims(skims_omx, data_dir=data_dir)\n    zone_nums = skims_omx.root.lookup.zone_number\n    cols = {}\n    for k in poi:\n        ktaz = poi[k][taz_col]\n        for name in names:\n            cols[f\"{k}_{name}\"] = pd.Series(\n                skims_omx.root.data[name][ktaz - 1],\n                index=zone_nums[:],\n            )\n    add_to_gdf = {}\n    if taz_col in gdf:\n        gdf_taz_col = gdf[taz_col]\n    elif gdf.index.name == taz_col:\n        gdf_taz_col = pd.Series(data=gdf.index, index=gdf.index)\n    else:\n        raise KeyError(taz_col)\n    for c in cols:\n        add_to_gdf[c] = gdf_taz_col.map(cols[c])\n    if cluster_factors is None:\n        cluster_factors = {}\n    new_cluster_factors = {\n        f\"{i}_{j}\": names[j] for i, j in itertools.product(poi.keys(), names.keys())\n    }\n    return gdf.assign(**add_to_gdf), cluster_factors | new_cluster_factors\n</code></pre>"},{"location":"api.html#rsm.sampler.rsm_household_sampler","title":"<code>rsm_household_sampler(input_dir='.', output_dir='.', prev_iter_access=None, curr_iter_access=None, study_area=None, input_household='households.csv', input_person='persons.csv', taz_crosswalk='taz_crosswalk.csv', mgra_crosswalk='mgra_crosswalk.csv', compare_access_columns=('NONMAN_AUTO', 'NONMAN_TRANSIT', 'NONMAN_NONMOTOR', 'NONMAN_SOV_0'), default_sampling_rate=0.25, lower_bound_sampling_rate=0.15, upper_bound_sampling_rate=1.0, random_seed=42, output_household='sampled_households.csv', output_person='sampled_person.csv')</code>","text":"<p>Take an intelligent sampling of households.</p>"},{"location":"api.html#rsm.sampler.rsm_household_sampler--parameters","title":"Parameters","text":"input_dir <p>default \u201c.\u201d</p> output_dir <p>default \u201c.\u201d</p> prev_iter_access <p>Accessibility in an old (default, no treatment, etc) run is given (preloaded) or read in from here. Give as a relative path (from <code>input_dir</code>) or an absolute path.</p> curr_iter_access <p>Accessibility in the latest run is given (preloaded) or read in from here. Give as a relative path (from <code>input_dir</code>) or an absolute path.</p> study_area <p>Array of RSM zone (these are numbered 1 to N in the RSM) in the study area. These zones are sampled at 100%.</p> input_household <p>Complete synthetic household file.  This data will be filtered to match the sampling of households and written out to a new CSV file.</p> input_person <p>Complete synthetic persons file.  This data will be filtered to match the sampling of households and written out to a new CSV file.</p> compare_access_columns <p>Column names in the accessibility file to use for comparing accessibility. Only changes in the values in these columns will be evaluated.</p> default_sampling_rate <p>The default sampling rate, in the range (0,1]</p> lower_bound_sampling_rate <p>Sampling rates by zone will be truncated so they are never lower than this.</p> upper_bound_sampling_rate <p>Sampling rates by zone will be truncated so they are never higher than this.</p>"},{"location":"api.html#rsm.sampler.rsm_household_sampler--returns","title":"Returns","text":"<p>sample_households_df, sample_persons_df : sample_households_df, sample_persons_df     These are the sampled population to resimulate.  They are also written to     the output_dir</p> Source code in <code>rsm/sampler.py</code> <pre><code>def rsm_household_sampler(\n    input_dir=\".\",\n    output_dir=\".\",\n    prev_iter_access=None,\n    curr_iter_access=None,\n    study_area=None,\n    input_household=\"households.csv\",\n    input_person=\"persons.csv\",\n    taz_crosswalk=\"taz_crosswalk.csv\",\n    mgra_crosswalk=\"mgra_crosswalk.csv\",\n    compare_access_columns=(\n        \"NONMAN_AUTO\",\n        \"NONMAN_TRANSIT\",\n        \"NONMAN_NONMOTOR\",\n        \"NONMAN_SOV_0\",\n    ),\n    default_sampling_rate=0.25,  # fix the values of this after some testing\n    lower_bound_sampling_rate=0.15,  # fix the values of this after some testing\n    upper_bound_sampling_rate=1.0,  # fix the values of this after some testing\n    random_seed=42,\n    output_household=\"sampled_households.csv\",\n    output_person=\"sampled_person.csv\",\n):\n\"\"\"\n    Take an intelligent sampling of households.\n\n    Parameters\n    ----------\n    input_dir : input_dir \n        default \".\"\n    output_dir : output_dir \n        default \".\"\n    prev_iter_access : prev_iter_access\n        Accessibility in an old (default, no treatment, etc) run is given (preloaded)\n        or read in from here. Give as a relative path (from `input_dir`) or an\n        absolute path.\n    curr_iter_access : curr_iter_access\n        Accessibility in the latest run is given (preloaded) or read in from here.\n        Give as a relative path (from `input_dir`) or an absolute path.\n    study_area : study_area\n        Array of RSM zone (these are numbered 1 to N in the RSM) in the study area. These zones are sampled at 100%.\n    input_household : input_household\n        Complete synthetic household file.  This data will be filtered to match the\n        sampling of households and written out to a new CSV file.\n    input_person : input_person\n        Complete synthetic persons file.  This data will be filtered to match the\n        sampling of households and written out to a new CSV file.\n    compare_access_columns : compare_access_columns\n        Column names in the accessibility file to use for comparing accessibility.\n        Only changes in the values in these columns will be evaluated.\n    default_sampling_rate : default_sampling_rate\n        The default sampling rate, in the range (0,1]\n    lower_bound_sampling_rate : lower_bound_sampling_rate\n        Sampling rates by zone will be truncated so they are never lower than this.\n    upper_bound_sampling_rate : upper_bound_sampling_rate\n        Sampling rates by zone will be truncated so they are never higher than this.\n\n    Returns\n    -------\n    sample_households_df, sample_persons_df : sample_households_df, sample_persons_df\n        These are the sampled population to resimulate.  They are also written to\n        the output_dir\n    \"\"\"\n\n    input_dir = Path(input_dir or \".\")\n    output_dir = Path(output_dir or \".\")\n\n    logger.debug(\"CALL rsm_household_sampler\")\n    logger.debug(f\"  {input_dir=}\")\n    logger.debug(f\"  {output_dir=}\")\n\n    def _resolve_df(x, directory, make_index=None):\n        if isinstance(x, (str, Path)):\n            # read in the file to a pandas DataFrame\n            x = Path(x).expanduser()\n            if not x.is_absolute():\n                x = Path(directory or \".\").expanduser().joinpath(x)\n            try:\n                result = pd.read_csv(x)\n            except FileNotFoundError:\n                raise\n        elif isinstance(x, pd.DataFrame):\n            result = x\n        elif x is None:\n            result = None\n        else:\n            raise TypeError(\"must be path-like or DataFrame\")\n        if (\n            result is not None\n            and make_index is not None\n            and make_index in result.columns\n        ):\n            result = result.set_index(make_index)\n        return result\n\n    def _resolve_out_filename(x):\n        x = Path(x).expanduser()\n        if not x.is_absolute():\n            x = Path(output_dir).expanduser().joinpath(x)\n        x.parent.mkdir(parents=True, exist_ok=True)\n        return x\n\n    prev_iter_access_df = _resolve_df(\n        prev_iter_access, input_dir, make_index=\"MGRA\"\n    )\n    curr_iter_access_df = _resolve_df(\n        curr_iter_access, input_dir, make_index=\"MGRA\"\n    )\n    rsm_zones = _resolve_df(taz_crosswalk, input_dir)\n    dict_clusters = dict(zip(rsm_zones[\"taz\"], rsm_zones[\"cluster_id\"]))\n\n    rsm_mgra_zones = _resolve_df(mgra_crosswalk, input_dir)\n    rsm_mgra_zones.columns = rsm_mgra_zones.columns.str.strip().str.lower()\n    dict_clusters_mgra = dict(zip(rsm_mgra_zones[\"mgra\"], rsm_mgra_zones[\"cluster_id\"]))\n\n    # changing the taz and mgra to new cluster ids\n    input_household_df = _resolve_df(input_household, input_dir)\n    input_household_df[\"taz\"] = input_household_df[\"taz\"].map(dict_clusters)\n    input_household_df[\"mgra\"] = input_household_df[\"mgra\"].map(dict_clusters_mgra)\n    input_household_df[\"count\"] = 1\n\n    mgra_hh = input_household_df.groupby([\"mgra\"]).size().rename(\"n_hh\").to_frame()\n\n    if curr_iter_access_df is None or prev_iter_access_df is None:\n\n        if curr_iter_access_df is None:\n            logger.warning(f\"missing curr_iter_access_df from {curr_iter_access}\")\n        if prev_iter_access_df is None:\n            logger.warning(f\"missing prev_iter_access_df from {prev_iter_access}\")\n        # true when sampler is turned off. default_sampling_rate should be set to 1\n\n        mgra_hh[\"sampling_rate\"] = default_sampling_rate\n        if study_area is not None:\n            mgra_hh.loc[mgra_hh.index.isin(study_area), \"sample_rate\"] = 1\n\n        sample_households = []\n\n        for mgra_id, row in mgra_hh.iterrows():\n            df = input_household_df.loc[input_household_df[\"mgra\"] == mgra_id]\n            sampling_rate = row[\"sampling_rate\"]\n            logger.info(f\"Sampling rate of RSM zone {mgra_id}: {sampling_rate}\")\n            df = df.sample(frac=sampling_rate, random_state=mgra_id + random_seed)\n            sample_households.append(df)\n\n        # combine study are and non-study area households into single dataframe\n        sample_households_df = pd.concat(sample_households)\n\n    else:\n        # restrict to rows only where TAZs have households\n        prev_iter_access_df = prev_iter_access_df[\n            prev_iter_access_df.index.isin(mgra_hh.index)\n        ].copy()\n        curr_iter_access_df = curr_iter_access_df[\n            curr_iter_access_df.index.isin(mgra_hh.index)\n        ].copy()\n\n        # compare accessibility columns\n        compare_results = pd.DataFrame()\n\n        for column in compare_access_columns:\n            compare_results[column] = (\n                curr_iter_access_df[column] - prev_iter_access_df[column]\n            ).abs()  # take absolute difference\n        compare_results[\"MGRA\"] = prev_iter_access_df.index\n\n        compare_results = compare_results.set_index(\"MGRA\")\n\n        # Take row sums of all difference\n        compare_results[\"Total\"] = compare_results[list(compare_access_columns)].sum(\n            axis=1\n        )\n\n        # TODO: potentially adjust this later after we figure out a better approach\n        wgts = compare_results[\"Total\"] + 0.01\n        wgts /= wgts.mean() / default_sampling_rate\n        compare_results[\"sampling_rate\"] = np.clip(\n            wgts, lower_bound_sampling_rate, upper_bound_sampling_rate\n        )\n\n        sample_households = []\n        sample_rate_df = compare_results[[\"sampling_rate\"]].copy()\n        if study_area is not None:\n            sample_rate_df.loc[\n                sample_rate_df.index.isin(study_area), \"sampling_rate\"\n            ] = 1\n\n        for mgra_id, row in sample_rate_df.iterrows():\n            df = input_household_df.loc[input_household_df[\"mgra\"] == mgra_id]\n            sampling_rate = row[\"sampling_rate\"]\n            logger.info(f\"Sampling rate of RSM zone {mgra_id}: {sampling_rate}\")\n            df = df.sample(frac=sampling_rate, random_state=mgra_id + random_seed)\n            sample_households.append(df)\n\n        # combine study are and non-study area households into single dataframe\n        sample_households_df = pd.concat(sample_households)\n\n    sample_households_df = sample_households_df.sort_values(by=[\"hhid\"])\n    sample_households_df.to_csv(_resolve_out_filename(output_household), index=False)\n\n    # select persons belonging to sampled households\n    sample_hhids = sample_households_df[\"hhid\"].to_numpy()\n\n    persons_df = _resolve_df(input_person, input_dir)\n    sample_persons_df = persons_df.loc[persons_df[\"hhid\"].isin(sample_hhids)]\n    sample_persons_df.to_csv(_resolve_out_filename(output_person), index=False)\n\n    global_sample_rate = round(len(sample_households_df) / len(input_household_df),2)\n    logger.info(f\"Total Sampling Rate : {global_sample_rate}\")\n\n    return sample_households_df, sample_persons_df\n</code></pre>"},{"location":"api.html#rsm.translate.copy_transit_demand","title":"<code>copy_transit_demand(matrix_names, input_dir='.', output_dir='.')</code>","text":"<p>copies the omx transit demand matrix to rsm directory</p>"},{"location":"api.html#rsm.translate.copy_transit_demand--parameters","title":"Parameters","text":"matrix_names <p>omx matrix filenames to aggregate</p> input_dir <p>default \u201c.\u201d</p> output_dir <p>default \u201c.\u201d</p>"},{"location":"api.html#rsm.translate.copy_transit_demand--returns","title":"Returns","text":"Source code in <code>rsm/translate.py</code> <pre><code>def copy_transit_demand(\n    matrix_names,\n    input_dir=\".\",\n    output_dir=\".\"\n):\n\"\"\"\n    copies the omx transit demand matrix to rsm directory\n\n    Parameters\n    ----------\n    matrix_names : matrix_names\n        omx matrix filenames to aggregate\n    input_dir : input_dir\n        default \".\"\n    output_dir : output_dir\n        default \".\"\n\n    Returns\n    -------\n\n    \"\"\"\n\n\n    for mat_name in matrix_names:\n        if '.omx' not in mat_name:\n            mat_name = mat_name + \".omx\"\n\n        input_file_dir = os.path.join(input_dir, mat_name)\n        output_file_dir = os.path.join(output_dir, mat_name)\n\n        shutil.copy(input_file_dir, output_file_dir)\n</code></pre>"},{"location":"api.html#rsm.translate.translate_emmebank_demand","title":"<code>translate_emmebank_demand(input_databank, output_databank, cores_to_aggregate, agg_zone_mapping)</code>","text":"<p>aggregates the demand matrix cores from one emme databank and loads them into another databank</p>"},{"location":"api.html#rsm.translate.translate_emmebank_demand--parameters","title":"Parameters","text":"input_databank <p>Emme databank</p> output_databank <p>Emme databank</p> cores_to_aggregate <p>matrix corenames to aggregate</p> agg_zone_mapping <p>zone number mapping between original and aggregated zones.  columns: original zones as \u2018taz\u2019 and aggregated zones as \u2018cluster_id\u2019</p>"},{"location":"api.html#rsm.translate.translate_emmebank_demand--returns","title":"Returns","text":"<p>None. Loads the trip matrices into emmebank.</p> Source code in <code>rsm/translate.py</code> <pre><code>def translate_emmebank_demand(\n    input_databank,\n    output_databank,\n    cores_to_aggregate,\n    agg_zone_mapping,\n): \n\"\"\"\n    aggregates the demand matrix cores from one emme databank and loads them into another databank\n\n    Parameters\n    ----------\n    input_databank : input_databank\n        Emme databank\n    output_databank : output_databank\n        Emme databank\n    cores_to_aggregate : cores_to_aggregate\n        matrix corenames to aggregate\n    agg_zone_mapping: agg_zone_mapping\n        zone number mapping between original and aggregated zones. \n        columns: original zones as 'taz' and aggregated zones as 'cluster_id'\n\n    Returns\n    -------\n    None. Loads the trip matrices into emmebank.\n\n    \"\"\"\n\n    agg_zone_mapping_df = pd.read_csv(os.path.join(agg_zone_mapping))\n    agg_zone_mapping_df = agg_zone_mapping_df.sort_values('taz')\n\n    agg_zone_mapping_df.columns= agg_zone_mapping_df.columns.str.strip().str.lower()\n    zone_mapping = dict(zip(agg_zone_mapping_df['taz'], agg_zone_mapping_df['cluster_id']))\n\n    for core in cores_to_aggregate: \n        matrix = input_databank.matrix(core).get_data()\n        matrix_array = matrix.to_numpy()\n\n        matrix_agg = _aggregate_matrix(matrix_array, zone_mapping)\n\n        output_matrix = output_databank.matrix(core)\n        output_matrix.set_numpy_data(matrix_agg)\n</code></pre>"},{"location":"api.html#rsm.translate.translate_omx_demand","title":"<code>translate_omx_demand(matrix_names, agg_zone_mapping, input_dir='.', output_dir='.')</code>","text":"<p>aggregates the omx demand matrix to aggregated zone system</p>"},{"location":"api.html#rsm.translate.translate_omx_demand--parameters","title":"Parameters","text":"matrix_names <p>omx matrix filenames to aggregate</p> agg_zone_mapping <p>zone number mapping between original and aggregated zones.  columns: original zones as \u2018taz\u2019 and aggregated zones as \u2018cluster_id\u2019</p> input_dir <p>default \u201c.\u201d</p> output_dir <p>default \u201c.\u201d</p>"},{"location":"api.html#rsm.translate.translate_omx_demand--returns","title":"Returns","text":"Source code in <code>rsm/translate.py</code> <pre><code>def translate_omx_demand(\n    matrix_names,\n    agg_zone_mapping,\n    input_dir=\".\",\n    output_dir=\".\"\n): \n\"\"\"\n    aggregates the omx demand matrix to aggregated zone system\n\n    Parameters\n    ----------\n    matrix_names : matrix_names\n        omx matrix filenames to aggregate\n    agg_zone_mapping: agg_zone_mapping\n        zone number mapping between original and aggregated zones. \n        columns: original zones as 'taz' and aggregated zones as 'cluster_id'\n    input_dir : input_dir\n        default \".\"\n    output_dir : output_dir \n        default \".\"\n\n    Returns\n    -------\n\n    \"\"\"\n\n    agg_zone_mapping_df = pd.read_csv(os.path.join(agg_zone_mapping))\n    agg_zone_mapping_df = agg_zone_mapping_df.sort_values('taz')\n\n    agg_zone_mapping_df.columns= agg_zone_mapping_df.columns.str.strip().str.lower()\n    zone_mapping = dict(zip(agg_zone_mapping_df['taz'], agg_zone_mapping_df['cluster_id']))\n    agg_zones = sorted(agg_zone_mapping_df['cluster_id'].unique())\n\n    for mat_name in matrix_names:\n        if '.omx' not in mat_name:\n            mat_name = mat_name + \".omx\"\n\n        #logger.info(\"Aggregating Matrix: \" + mat_name + \" ...\")\n\n        input_skim_file = os.path.join(input_dir, mat_name)\n        print(input_skim_file)\n        output_skim_file = os.path.join(output_dir, mat_name)\n\n        assert os.path.isfile(input_skim_file)\n\n        input_matrix = omx.open_file(input_skim_file, mode=\"r\") \n        input_mapping_name = input_matrix.list_mappings()[0]\n        input_cores = input_matrix.list_matrices()\n\n        output_matrix = omx.open_file(output_skim_file, mode=\"w\")\n\n        for core in input_cores:\n            matrix = input_matrix[core]\n            matrix_array = matrix.read()\n            matrix_agg = _aggregate_matrix(matrix_array, zone_mapping)\n            output_matrix[core] = matrix_agg\n\n        output_matrix.create_mapping(title=input_mapping_name, entries=agg_zones)\n\n        input_matrix.close()\n        output_matrix.close()\n</code></pre>"},{"location":"api.html#rsm.utility.ReplacementOfString","title":"<code>ReplacementOfString</code>","text":"<p>This class provides a mechanism to edit a file, replacing the string value of a particular parameter with a new value.</p> Source code in <code>rsm/utility.py</code> <pre><code>class ReplacementOfString:\n\"\"\"\n    This class provides a mechanism to edit a file, replacing\n    the string value of a particular parameter with a new value.\n    \"\"\"\n    def __init__(self, varname, assign_operator=\"=\"):\n        self.varname = varname\n        self.regex = re.compile(f\"({varname}\\s*{assign_operator}[ \\t\\f\\v]*)([^#\\n]*)(#.*)?\\n\", flags=re.MULTILINE)\n    def sub(self, value, s):\n        s, n = self.regex.subn(f\"\\g&lt;1&gt;{value}\\g&lt;3&gt;\\n\", s)\n        logger.info(f\"For '{self.varname}': {n} substitutions made\")\n        return s\n</code></pre>"},{"location":"api.html#rsm.utility.copy_file","title":"<code>copy_file(src, dest)</code>","text":"<p>Create copy of file</p> Source code in <code>rsm/utility.py</code> <pre><code>def copy_file(src, dest):\n\"\"\"\n    Create copy of file \n\n    \"\"\"\n    shutil.copy(src, dest)\n</code></pre>"},{"location":"api.html#rsm.utility.extract_number_in_filename","title":"<code>extract_number_in_filename(f)</code>","text":"<p>extrcats the number from the file name. It is used in extracting the iteration number from school and work shadow pricing files</p> Source code in <code>rsm/utility.py</code> <pre><code>def extract_number_in_filename(f):\n\"\"\"\n    extrcats the number from the file name.\n    It is used in extracting the iteration number from school and work shadow pricing files\n    \"\"\"\n    s = re.findall(\"\\d+\",f)\n    return (int(s[0]) if s else -1,f)\n</code></pre>"},{"location":"api.html#rsm.utility.fix_zero_enrollment","title":"<code>fix_zero_enrollment(mgra_df)</code>","text":"<p>adjusts the elementary and high school enrollments for RSM</p> Source code in <code>rsm/utility.py</code> <pre><code>def fix_zero_enrollment(mgra_df):\n\n\"\"\"\n    adjusts the elementary and high school enrollments for RSM\n\n    \"\"\"\n\n    ech_check = mgra_df.groupby(['ech_dist'])['enrollgradekto8'].sum().reset_index()\n    ech_dist_df = ech_check.loc[ech_check['enrollgradekto8']==0]\n    if len(ech_dist_df) &gt; 0:\n        ech_dist_mod = list(ech_dist_df['ech_dist'])\n        # print(ech_dist_mod)\n        mgra_df.loc[mgra_df['ech_dist'].isin(ech_dist_mod), 'enrollgradekto8'] = 99999\n\n    hch_check = mgra_df.groupby(['hch_dist'])['enrollgrade9to12'].sum().reset_index()\n    hch_dist_df = hch_check.loc[hch_check['enrollgrade9to12']==0]\n    if len(hch_dist_df) &gt; 0:\n        hch_dist_mod = list(hch_dist_df['hch_dist'])\n        # print(hch_dist_mod)\n        mgra_df.loc[mgra_df['hch_dist'].isin(ech_dist_mod), 'enrollgrade9to12'] = 99999\n\n    return mgra_df\n</code></pre>"},{"location":"api.html#rsm.utility.get_property","title":"<code>get_property(properties_file, property_name)</code>","text":"<p>Extracts the property_value for a property_name from sandag_abm.properties files</p> Source code in <code>rsm/utility.py</code> <pre><code>def get_property(properties_file, property_name):\n\"\"\"\n    Extracts the property_value for a property_name from sandag_abm.properties files\n    \"\"\"\n    with open(properties_file, \"r\") as f:\n        lines = f.readlines()\n        key_found = False\n        for line in lines:\n            if property_name in line:\n                key = line.split(\"=\")[0].strip()\n                value = line.split(\"=\")[1].strip()\n\n                if property_name == key:\n                    key_found = True\n                    break\n\n    if not key_found:\n        raise Exception(\"{} not found in sandag_abm.properties file\".format(property_name))\n\n    return value\n</code></pre>"},{"location":"api.html#rsm.utility.get_shadow_pricing_files","title":"<code>get_shadow_pricing_files(folder)</code>","text":"<p>folder is path to location of shadow pricing files.</p> Source code in <code>rsm/utility.py</code> <pre><code>def get_shadow_pricing_files(folder):\n\"\"\"\n    folder is path to location of shadow pricing files.\n    \"\"\"\n    sp_work_files = glob.glob(os.path.join(folder, 'ShadowPricingOutput_work*.csv'), recursive=True)\n    sp_sch_files = glob.glob(os.path.join(folder, 'ShadowPricingOutput_school*.csv'), recursive=True)\n\n    sp_work_files =  [os.path.split(x)[1] for x in sp_work_files]\n    sp_sch_files = [os.path.split(x)[1] for x in sp_sch_files]\n\n    sp_work_max = max(sp_work_files, key=extract_number_in_filename)\n    sp_school_max = max(sp_sch_files, key=extract_number_in_filename)\n\n    return sp_work_max, sp_school_max\n</code></pre>"},{"location":"api.html#rsm.utility.set_property","title":"<code>set_property(properties_file, property_name, property_value)</code>","text":"<p>Modifies the sandag properties file</p> Source code in <code>rsm/utility.py</code> <pre><code>def set_property(properties_file, property_name, property_value):\n\"\"\"\n    Modifies the sandag properties file\n\n    \"\"\"\n\n    with open(properties_file) as f:\n        y = f.read()\n\n    y = ReplacementOfString(property_name).sub(property_value, y)\n\n    with open(properties_file, 'wt') as f:\n        f.write(y)\n</code></pre>"},{"location":"api.html#rsm.zone_agg.aggregate_zones","title":"<code>aggregate_zones(mgra_gdf, method='kmeans', n_zones=2000, random_state=0, cluster_factors=None, cluster_factors_onehot=None, use_xy=True, explicit_agg=(), explicit_col='mgra', agg_instruction=None, start_cluster_ids=13)</code>","text":"<p>Aggregate zones.</p>"},{"location":"api.html#rsm.zone_agg.aggregate_zones--parameters","title":"Parameters","text":"GeoDataFrame <p>Geometry and attibutes of MGRAs</p> <p>method : {\u2018kmeans\u2019, \u2018agglom\u2019, \u2018agglom_adj\u2019} n_zones : int random_state : RandomState or int cluster_factors : dict cluster_factors_onehot : dict</p> bool or float <p>Use X and Y coordinates as a cluster factor, use a float to scale the x-y coordinates from the CRS if needed.</p> list[int or list] <p>A list containing integers (individual MGRAs that should not be aggregated) or lists of integers (groups of MGRAs that should be aggregated exactly as given, with no less and no more)</p> str <p>The name of the column containing the ID\u2019s from <code>explicit_agg</code>, usually \u2018mgra\u2019 or \u2018taz\u2019</p> dict <p>Dictionary passed to pandas <code>agg</code> that says how to aggregate data columns.</p> int, default 13 <p>Cluster id\u2019s start at this value.  Can be 1, but typically SANDAG has the smallest id\u2019s reserved for external zones, so starting at a greater value is typical.</p>"},{"location":"api.html#rsm.zone_agg.aggregate_zones--returns","title":"Returns","text":"<p>GeoDataFrame</p> Source code in <code>rsm/zone_agg.py</code> <pre><code>def aggregate_zones(\n    mgra_gdf,\n    method=\"kmeans\",\n    n_zones=2000,\n    random_state=0,\n    cluster_factors=None,\n    cluster_factors_onehot=None,\n    use_xy=True,\n    explicit_agg=(),\n    explicit_col=\"mgra\",\n    agg_instruction=None,\n    start_cluster_ids=13,\n):\n\"\"\"\n    Aggregate zones.\n\n    Parameters\n    ----------\n    mgra_gdf : GeoDataFrame\n        Geometry and attibutes of MGRAs\n    method : {'kmeans', 'agglom', 'agglom_adj'}\n    n_zones : int\n    random_state : RandomState or int\n    cluster_factors : dict\n    cluster_factors_onehot : dict\n    use_xy : bool or float\n        Use X and Y coordinates as a cluster factor, use a float to scale the\n        x-y coordinates from the CRS if needed.\n    explicit_agg : list[int or list]\n        A list containing integers (individual MGRAs that should not be aggregated)\n        or lists of integers (groups of MGRAs that should be aggregated exactly as\n        given, with no less and no more)\n    explicit_col : str\n        The name of the column containing the ID's from `explicit_agg`, usually\n        'mgra' or 'taz'\n    agg_instruction : dict\n        Dictionary passed to pandas `agg` that says how to aggregate data columns.\n    start_cluster_ids : int, default 13\n        Cluster id's start at this value.  Can be 1, but typically SANDAG has the\n        smallest id's reserved for external zones, so starting at a greater value\n        is typical.\n\n    Returns\n    -------\n    GeoDataFrame\n    \"\"\"\n\n    if cluster_factors is None:\n        cluster_factors = {}\n\n    n = start_cluster_ids\n    if explicit_agg:\n        explicit_agg_ids = {}\n        for i in explicit_agg:\n            if isinstance(i, Number):\n                explicit_agg_ids[i] = n\n            else:\n                for j in i:\n                    explicit_agg_ids[j] = n\n            n += 1\n        if explicit_col == mgra_gdf.index.name:\n            mgra_gdf = mgra_gdf.reset_index()\n            mgra_gdf.index = mgra_gdf[explicit_col]\n        in_explicit = mgra_gdf[explicit_col].isin(explicit_agg_ids)\n        mgra_gdf_algo = mgra_gdf.loc[~in_explicit].copy()\n        mgra_gdf_explicit = mgra_gdf.loc[in_explicit].copy()\n        mgra_gdf_explicit[\"cluster_id\"] = mgra_gdf_explicit[explicit_col].map(\n            explicit_agg_ids\n        )\n        n_zones_algorithm = n_zones - len(\n            mgra_gdf_explicit[\"cluster_id\"].value_counts()\n        )\n    else:\n        mgra_gdf_algo = mgra_gdf.copy()\n        mgra_gdf_explicit = None\n        n_zones_algorithm = n_zones\n\n    if use_xy:\n        geometry = mgra_gdf_algo.centroid\n        X = list(geometry.apply(lambda p: p.x))\n        Y = list(geometry.apply(lambda p: p.y))\n        factors = [np.asarray(X) * use_xy, np.asarray(Y) * use_xy]\n    else:\n        factors = []\n    for cf, cf_wgt in cluster_factors.items():\n        factors.append(cf_wgt * mgra_gdf_algo[cf].values.astype(np.float32))\n    if cluster_factors_onehot:\n        for cf, cf_wgt in cluster_factors_onehot.items():\n            factors.append(cf_wgt * OneHotEncoder().fit_transform(mgra_gdf_algo[[cf]]))\n        from scipy.sparse import hstack\n\n        factors2d = []\n        for j in factors:\n            if j.ndim &lt; 2:\n                factors2d.append(np.expand_dims(j, -1))\n            else:\n                factors2d.append(j)\n        data = hstack(factors2d).toarray()\n    else:\n        data = np.array(factors).T\n\n    if method == \"kmeans\":\n        kmeans = KMeans(n_clusters=n_zones_algorithm, random_state=random_state)\n        kmeans.fit(data)\n        cluster_id = kmeans.labels_\n    elif method == \"agglom\":\n        agglom = AgglomerativeClustering(\n            n_clusters=n_zones_algorithm, affinity=\"euclidean\", linkage=\"ward\"\n        )\n        agglom.fit_predict(data)\n        cluster_id = agglom.labels_\n    elif method == \"agglom_adj\":\n        from libpysal.weights import Rook\n\n        w_rook = Rook.from_dataframe(mgra_gdf_algo)\n        adj_mat = nx.adjacency_matrix(w_rook.to_networkx())\n        agglom = AgglomerativeClustering(\n            n_clusters=n_zones_algorithm,\n            affinity=\"euclidean\",\n            linkage=\"ward\",\n            connectivity=adj_mat,\n        )\n        agglom.fit_predict(data)\n        cluster_id = agglom.labels_\n    else:\n        raise NotImplementedError(method)\n    mgra_gdf_algo[\"cluster_id\"] = cluster_id\n\n    if mgra_gdf_explicit is None or len(mgra_gdf_explicit) == 0:\n        combined = merge_zone_data(\n            mgra_gdf_algo,\n            agg_instruction,\n            cluster_id=\"cluster_id\",\n        )\n        combined[\"cluster_id\"] = list(range(n, n + n_zones_algorithm))\n    else:\n        pending = []\n        for df in [mgra_gdf_algo, mgra_gdf_explicit]:\n            logger.info(f\"... merging {len(df)}\")\n            pending.append(\n                merge_zone_data(\n                    df,\n                    agg_instruction,\n                    cluster_id=\"cluster_id\",\n                ).reset_index()\n            )\n\n        pending[0][\"cluster_id\"] = list(range(n, n + n_zones_algorithm))\n\n        pending[0] = pending[0][\n            [c for c in pending[1].columns if c in pending[0].columns]\n        ]\n        pending[1] = pending[1][\n            [c for c in pending[0].columns if c in pending[1].columns]\n        ]\n        combined = pd.concat(pending, ignore_index=False)\n    combined = combined.reset_index(drop=True)\n\n    return combined\n</code></pre>"},{"location":"assessment.html","title":"Assessment","text":"<p>RSM performance assessment results</p>"},{"location":"assessment.html#runtime","title":"Runtime","text":""},{"location":"assessment.html#validation","title":"Validation","text":""},{"location":"assessment.html#elasticity-comparison-aoc-plus-50","title":"Elasticity Comparison - AOC - plus 50%","text":""},{"location":"assessment.html#elasticity-comparison-aoc-minus-50","title":"Elasticity Comparison - AOC - minus 50%","text":""},{"location":"assessment.html#elasticity-comparison-job-hh-balance","title":"Elasticity Comparison - Job HH Balance","text":""},{"location":"assessment.html#elasticity-comparison-mixed-land-use","title":"Elasticity Comparison - Mixed Land Use","text":""},{"location":"assessment.html#elasticity-comparison-100-automated-vehicle-adoption","title":"Elasticity Comparison - 100% Automated Vehicle Adoption","text":""},{"location":"assessment.html#elasticity-comparison-ride-hailing-cost-minus-50","title":"Elasticity Comparison - Ride Hailing Cost - minus 50%","text":""},{"location":"assessment.html#elasticity-comparison-175-parking-surcharge-in-mobility-hubs","title":"Elasticity Comparison - $1.75 Parking Surcharge in Mobility Hubs","text":""},{"location":"assessment.html#elasticity-comparison-100-increase-in-transit-frequencies","title":"Elasticity Comparison - 100% Increase in Transit Frequencies","text":""},{"location":"assessment.html#elasticity-comparison-south-bay-expressway-toll-removal","title":"Elasticity Comparison - South Bay Expressway Toll Removal","text":""},{"location":"assessment.html#boarding-comparison-rapid-637-brt","title":"Boarding Comparison - Rapid 637 BRT","text":""},{"location":"development.html","title":"Development","text":""},{"location":"development.html#needs","title":"Needs","text":"<p>The time needed to configure, run, and summarize results from ABM2+ is too slow to support a nimble, challenging, and engagement-oriented planning process. SANDAG needs a tool that quickly approximates the outcomes of ABM2+. The rapid strategic model, or RSM, was built for this purpose.</p> <p>ABM2+ Schematic is shown below </p>"},{"location":"development.html#design","title":"Design","text":""},{"location":"development.html#reducing-the-number-of-zones-reduces-model-runtime","title":"Reducing the number of zones reduces model runtime.","text":"<ul> <li> <p>MGRAs are aggregated into Rapid Zones based on their proximity to each other and similarity in regards to mode choice decisions.</p> </li> <li> <p>Number of rapid zones can be quickly changed to assess trade-offs between runtime and how well the RSM results match the ABM2+ results.</p> </li> <li> <p>Initial testing revealed 2,000 rapid zones is approximately optimal and will be used in initial deployments. For reference, ABM2+ has ~23,000 MGRAs and ~5,000 TAZs.</p> </li> </ul>"},{"location":"development.html#reducing-the-number-of-model-components-reduces-runtime","title":"Reducing the number of model components reduces runtime.","text":"<ul> <li>Most, but not all, of the policies of interest to SANDAG primarily impact resident passenger travel. </li> </ul>"},{"location":"development.html#reducing-the-number-of-global-iterations-reduces-runtime","title":"Reducing the number of global iterations reduces runtime.","text":""},{"location":"development.html#abm2-vs-rsm","title":"ABM2+ vs RSM","text":""},{"location":"development.html#abm2-simulates-population","title":"ABM2+ simulates population:","text":"<ul> <li>First iteration: 25 percent</li> <li>Second iteration: 50 percent</li> <li>Third iteration: 100 percent</li> </ul>"},{"location":"development.html#rsm-simulates-population","title":"RSM simulates population:","text":"<ul> <li>Higher rate in zones with large changes in accessibiility</li> <li>Lower rates in zones with small changes in accessibility</li> <li>This attempts to mitigate the impact of reducing the number of global iterations from 3 to 2.     </li> </ul>"},{"location":"home.html","title":"SANDAG Rapid Strategic Model","text":"<p>Welcome to the SANDAG Rapid Strategic Model documentation site!</p>"},{"location":"home.html#introduction","title":"Introduction","text":"<p>The travel demand model SANDAG used for the 2021 regional plan, referred to as ABM2+, is one of the most sophisticated modeling tools used anywhere in the world. Its activity-based approach to representing travel is behaviorally rich; the representations of land development and transportation infrastructure are represented in high fidelity spatial detail. An operational shortcoming of ABM2+ is it requires significant computational resources to carry out a simulation. A typical forecast year simulation of ABM2+ takes over 40 hours to complete on a high end workstation (e.g., 48 physical computing cores and 256 gigabytes of RAM). The components of this runtime include: Three iterations of the resident activity-based model, each about 6 hours; and, Four iterations of roadway and transit assignment, with each iteration taking about 90 minutes. The computational time of ABM2+, and the likely computational time of the successor to ABM2+ (ABM3), hinders SANDAG\u2019s ability to carry out certain analyses in a timely manner. For example, if an analyst wants to explore 10 different roadway pricing schemes for a select corridor, a month of computation time would be required.</p> <p>SANDAG needs a tool that quickly approximates the outcomes of ABM2+. We will therefore build such a tool, which will be referred to henceforth as the Rapid Strategic Model, or RSM. In creating the tool, we will also be mindful of other limitations of ABM2+ and ABM3, including simulating policies that are currently tedious to represent in the models\u2019 framework. The primary purpose of the RSM is to improve the speed of the resident passenger component of the broader modeling system. However, we will be mindful of opportunities outside of the resident passenger component to improve SANDAG\u2019s analytical planning workflow during the RSM project.</p>"},{"location":"userguide.html","title":"User Guide","text":"<p>TODO: document user guide here</p>"},{"location":"visualizer.html","title":"visualizer","text":"<p>TODO: document visualizer here</p>"}]}